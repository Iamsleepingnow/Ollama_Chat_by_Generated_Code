import json
import random
import time
import atexit
import os
import ollama
import gradio as gr
from datetime import datetime
import webbrowser

# é»˜è®¤æ¨¡å‹é…ç½®ä¿¡æ¯ï¼ˆå‚è€ƒï¼‰ä¸€èˆ¬æƒ…å†µä¸‹ä¼šè¢«./Configs.jsonè¦†ç›–
configs = {
    # IPåœ°å€
    "host": "127.0.0.1",
    # ç«¯å£å·
    "port": "11434",
    # UIç«¯å£
    "uiport": "7866",
    # æ¨¡å‹åç§°
    "model_name": "qwq-7b",
    # ç³»ç»Ÿæç¤ºè¯
    "system_prompt": "## Role:\n"
                     "æˆ‘æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘ç²¾é€šå¤šå›½è¯­è¨€ï¼Œå¯¹ç§‘å­¦ã€æ–‡å­¦ã€å†å²ã€æ•°å­¦ã€å“²å­¦ã€è‰ºæœ¯ç­‰æ–‡åŒ–é¢†åŸŸäº†å¦‚æŒ‡æŒã€‚è¯·å‘Šè¯‰æˆ‘ä½ çš„éœ€æ±‚ï¼Œæˆ‘éƒ½èƒ½å°½æˆ‘æ‰€èƒ½åœ°æ‰§è¡Œå¹¶ç»™å‡ºå»ºè®®ã€‚\n"
                     "## Background:\n"
                     "ç°å¦‚ä»Šï¼Œéšç€äººä»¬ç”Ÿæ´»æ°´å¹³çš„æé«˜ï¼Œåœ¨ç”Ÿæ´»ä¸­è·å–çš„ä¿¡æ¯ä¹Ÿæ›´åŠ çš„ç¢ç‰‡åŒ–ã€‚æˆ‘ä½œä¸ºæ™ºèƒ½åŠ©æ‰‹ä¼šå¸®åŠ©ä»–ä»¬æ•´åˆçŸ¥è¯†ä¿¡æ¯ï¼Œå¯¹çŸ¥è¯†è¿›è¡Œè¿‡æ»¤ã€ç­›é€‰ï¼Œæœ€åå¤„ç†æˆç®€ç»ƒä¸”æ˜“äºé˜…è¯»çš„æ–‡æœ¬è¿›è¡Œå›ç­”ï¼Œè¿™å¯¹ä»–ä»¬éå¸¸é‡è¦ï¼Œæˆ‘ä¼šåŠªåŠ›æä¾›æ›´å¥½çš„è§£è¯»ï¼Œå®ç°æ›´å¥½çš„åˆ›æ–°ã€‚\n"
                     "## Goal:\n"
                     "ç»“åˆæ‰€å­¦çŸ¥è¯†ä¸ç”¨æˆ·æ‰€æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™å‡ºç­”æ¡ˆæˆ–æ–‡æœ¬ç”Ÿæˆã€‚\n"
                     "åœ¨å¿…è¦æ—¶å€™æŠŠä¸“ä¸šè¯æ±‡ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è¿›è¡Œè§£è¯»ã€‚\n"
                     "åœ¨é€‚å½“çš„æ—¶å€™å¯ä»¥å¼€ç©ç¬‘ï¼Œä½†ä¸è¦å¤ªè¿‡åˆ†ã€‚\n"
                     "## Constrains:\n"
                     "å¦‚æœç”¨æˆ·æ²¡æœ‰ä»»ä½•è¯­è¨€ä¸Šçš„è¦æ±‚ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ç®€ä½“ä¸­æ–‡äº¤æµã€‚\n"
                     "é¿å…å‡ºç°è¿‡å¤šé‡å¤æ–‡æœ¬ï¼Œè¾“å‡ºå†…å®¹å°½é‡ç¾è§‚ï¼Œè¿›è¡Œåˆé€‚çš„Markdownæ’ç‰ˆï¼Œä¾‹å¦‚åœ¨ç‰¹æ®Šå…³é”®è¯ä¸ŠåŠ ç²—æˆ–æ”¾å¤§ï¼Œåœ¨è¾“å‡ºé¦–å°¾éœ€è¦æ¢è¡Œã€‚\n"
                     "## Skills:\n"
                     "è‡ªç„¶ç§‘å­¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼ŒåŒ…æ‹¬ç»å…¸ç‰©ç†ã€é‡å­åŠ›å­¦ã€ç”Ÿç‰©å­¦ã€åŒ–å­¦ã€æ•°å­¦ç­‰ã€‚\n"
                     "ç¤¾ä¼šç§‘å­¦é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼ŒåŒ…æ‹¬äººç±»å­¦ã€å¿ƒç†å­¦ã€å†å²å­¦ã€è‰ºæœ¯å­¦ã€è¯­è¨€å­¦ç­‰ã€‚\n"
                     "ä¼˜ç§€çš„è¯­è¨€è¡¨è¾¾èƒ½åŠ›ï¼Œèƒ½å¯¹ä¸“ä¸šè¯æ±‡è¿›è¡Œå‡†ç¡®ä¸”é€šä¿—çš„è§£é‡Šï¼Œå¯¹è¯ä¸å¤±é£è¶£å¹½é»˜ã€‚",
    # åŠ©æ‰‹ç¬¬ä¸€å¥æç¤ºè¯
    "assistant_first_prompt": "æˆ‘æ˜¯ä½ çš„å°åŠ©æ‰‹ï¼Œä½ å¯ä»¥å«æˆ‘å°åŠ©ï¼Œæœ‰ä»€ä¹ˆéœ€è¦å¸®åŠ©çš„å—ï¼ŸğŸ˜‹",
    # æ¨¡å‹å‚æ•°é€‰é¡¹
    "options": {
        "temperature": 1.0,  # temperatureå€¼è¶Šé«˜ï¼Œæ¨¡å‹åˆ›é€ æ€§è¶Šå¼ºï¼›å€¼è¶Šä½ï¼Œæ¨¡å‹ç›¸å¹²æ€§è¶Šå¼ºã€‚å€¼1ä¸ºé»˜è®¤å€¼ã€‚
        "num_ctx": 4096,  # num_ctxæ˜¯ä¸Šä¸‹æ–‡æœ€å¤§tokenæ•°é‡ï¼Œé»˜è®¤ä¸º4096ã€‚
        "top_k": 50,  # top_ké‡‡æ ·ä¼šå½±å“å›ç­”çš„å¤šæ ·åŒ–ç¨‹åº¦ï¼Œæ¨èèŒƒå›´10~100ï¼Œé»˜è®¤40ã€‚
        "top_p": 0.9,  # top_Pç›¸å½“äºtop_Kçš„é˜ˆå€¼ï¼Œä¹Ÿä¼šå½±å“å¤šæ ·åŒ–ç¨‹åº¦ï¼Œæ¨èèŒƒå›´0.5~0.95ï¼Œé»˜è®¤0.9ã€‚
        "repeat_penalty": 1.2,  # repeat_penaltyæ˜¯é‡å¤æƒ©ç½šï¼Œèƒ½é¿å…ç”Ÿæˆé‡å¤ä¿¡æ¯ï¼Œæ¨èèŒƒå›´0.9~1.5ï¼Œé»˜è®¤1.1ã€‚
        "seed": -1  # seedæ˜¯ä½¿ç”¨å›ºå®šç§å­ï¼Œ-1ä¸ºä½¿ç”¨éšæœºç§å­ã€‚
    }
}

# åŠ è½½é…ç½®
with open('./Configs.json', 'r', encoding='utf-8') as f:
    conf = json.load(f)
    configs["host"] = conf["host"]
    configs["port"] = conf["port"]
    configs["uiport"] = conf["uiport"]
    configs["model_name"] = conf["model_name"]
    configs["system_prompt"] = conf["system_prompt"]
    configs["assistant_first_prompt"] = conf["assistant_first_prompt"]
    configs["options"] = conf["options"]

# å¯¹è¯å†å²
history = []

# å…¨å±€æ ‡å¿—ï¼Œç”¨äºä¸­æ–­æ¨¡å‹è¾“å‡º
stop_generation = False


def model_history_restart():  # æ¨¡å‹å†å²é‡ç½®
    history.clear()
    history.append({"role": "system", "content": configs['system_prompt']})
    history.append({"role": "assistant", "content": configs['assistant_first_prompt']})
    return history


def chat_to_ollama(user_input):  # ä¸ollamaèŠå¤©ï¼Œè¿”å›èŠå¤©è¿­ä»£å™¨
    global stop_generation
    stop_generation = False
    history.append({"role": "user", "content": user_input})
    response = ollama.Client(host=f"http://{configs['host']}:{configs['port']}").chat(
        model=configs['model_name'], stream=True, messages=history, options={
            "temperature": configs['options']['temperature'],
            "num_ctx": configs['options']['num_ctx'],
            "top_k": configs['options']['top_k'],
            "top_p": configs['options']['top_p'],
            "repeat_penalty": configs['options']['repeat_penalty'],
            "seed": configs['options']["seed"] if configs['options']["seed"] >= 0 else random.randint(0, 1000000000)})

    assistant_response = ""
    for chunk in response:
        if stop_generation:
            break
        assistant_response += chunk['message']['content']
        yield assistant_response
    if not stop_generation:
        history.append({"role": "assistant", "content": assistant_response})


def stop_at_exit():  # é€€å‡º
    # å½“ç”¨æˆ·é€€å‡ºåº”ç”¨æ—¶ï¼Œæ‰§è¡Œæ§åˆ¶å°å‘½ä»¤ï¼šollama stop <æ¨¡å‹åç§°>
    os.system(f'ollama stop {configs["model_name"]}')
    print('\n<- æ­£åœ¨é€€å‡º ->\n')
    time.sleep(1.5)


def save_history():
    # ç”Ÿæˆéšæœºæ–‡ä»¶å
    random_str = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=6))
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"History_{random_str}_{current_time}.json"
    # ä¿å­˜å†å²åˆ°æ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=4, ensure_ascii=False)
    return filename


def load_history(file):
    try:
        with open(file.name, 'r', encoding='utf-8') as f:
            loaded_history = json.load(f)
        history.clear()
        history.extend(loaded_history)
        return loaded_history
    except Exception as e:
        print(f"åŠ è½½å†å²å¤±è´¥: {e}")
        return None


def update_config(temperature, top_k, top_p, repeat_penalty, seed):
    configs['options']['temperature'] = temperature
    configs['options']['top_k'] = top_k
    configs['options']['top_p'] = top_p
    configs['options']['repeat_penalty'] = repeat_penalty
    configs['options']['seed'] = seed


def random_seed():
    return random.randint(0, 1000000000)


def stop_generation_fn():
    global stop_generation
    stop_generation = True


# Gradioç•Œé¢
with gr.Blocks(
        css=".gradio-container {background-color: #252A34;}"
            ".chatbot {background-color: #08D9D6; overflow: auto;}"
            ".top-panel {display: none;}"
            ".btn-submit {background-color: #FF2E63; height: 95px; max_height: 95px; font-size: 30px;}"
            ".btn-normal {background-color: #08D9D6;}"
            ".btn-seed {height: 100px;}"
            ".txt-chat-input {height: 150px; max_height: 150px;}"
) as demo:
    # æ ‡é¢˜
    gr.HTML("<h1 align='center' class='normal-text'>ğŸ”¥ Ollama å¯¹è¯æœºå™¨äºº ğŸ”¥</h1>")

    # èŠå¤©ç•Œé¢
    chatbot = gr.Chatbot(type="messages", label=f"{configs['model_name']}ï¼š", value=model_history_restart(),
                         editable="all", height=500,
                         show_copy_button=True, avatar_images=(None, None), resizeable=True,
                         min_height=200)  # åˆå§‹åŒ–æ—¶åŠ è½½åŠ©æ‰‹çš„ç¬¬ä¸€å¥è¯
    with gr.Row():
        msg = gr.Textbox(label="è¾“å…¥æ¶ˆæ¯", container=False, lines=7, max_lines=20,
                         placeholder=">>> è¯´ç‚¹ä»€ä¹ˆå§", scale=6, elem_classes="txt-chat-input")
        # åŠŸèƒ½æŒ‰é’®
        with gr.Column(scale=1):
            submit_btn = gr.Button("æäº¤", variant="primary", scale=1, elem_classes="btn-submit")
            stop_btn = gr.Button("ä¸­æ­¢", scale=1, elem_classes="btn-normal")
        with gr.Column(scale=1, min_width=50):
            clear_btn = gr.Button("æ¸…é™¤å†å²", scale=1, elem_classes="btn-normal")
            save_btn = gr.Button("ä¿å­˜å†å²", scale=1, elem_classes="btn-normal")
            load_btn = gr.UploadButton("è¯»å–å†å²", file_types=[".json"], scale=1, elem_classes="btn-normal")

    # æ–‡ä»¶ä¿å­˜ç»„ä»¶
    file_save = gr.File(visible=False)

    # æ¨¡å‹é…ç½®
    with gr.Accordion("æ¨¡å‹é…ç½®", open=False):
        temperature = gr.Slider(0.0, 2.0, value=configs['options']['temperature'], label="Temperature",
                                info="temperatureå€¼è¶Šé«˜ï¼Œæ¨¡å‹åˆ›é€ æ€§è¶Šå¼ºï¼›å€¼è¶Šä½ï¼Œæ¨¡å‹ç›¸å¹²æ€§è¶Šå¼ºã€‚")
        top_k = gr.Slider(10, 100, value=configs['options']['top_k'], step=1, label="Top_k",
                          info="top_ké‡‡æ ·ä¼šå½±å“å›ç­”çš„å¤šæ ·åŒ–ç¨‹åº¦ã€‚")
        top_p = gr.Slider(0.5, 0.95, value=configs['options']['top_p'], label="Top_p",
                          info="top_Pç›¸å½“äºtop_Kçš„é˜ˆå€¼ï¼Œä¹Ÿä¼šå½±å“å¤šæ ·åŒ–ç¨‹åº¦ã€‚")
        repeat_penalty = gr.Slider(0.8, 1.5, value=configs['options']['repeat_penalty'], label="Repeat penalty",
                                   info="repeat_penaltyæ˜¯é‡å¤æƒ©ç½šï¼Œèƒ½é¿å…ç”Ÿæˆé‡å¤ä¿¡æ¯ã€‚")
        with gr.Row():
            seed = gr.Number(value=configs['options']['seed'], label="Seed",
                             info="seedæ˜¯ä½¿ç”¨å›ºå®šç§å­ï¼Œ-1ä¸ºä½¿ç”¨éšæœºç§å­ã€‚", scale=5)
            random_seed_btn = gr.Button("éšæœº", scale=1, elem_classes="btn-seed")

    # äº¤äº’é€»è¾‘
    def respond(user_input, chat_history):
        bot_response = chat_to_ollama(user_input)
        chat_history.append({"role": "user", "content": user_input})  # æ·»åŠ ç”¨æˆ·è¾“å…¥åˆ°èŠå¤©è®°å½•
        for response in bot_response:
            if len(chat_history) > 0 and chat_history[-1]["role"] == "user":
                chat_history.append({"role": "assistant", "content": response})  # æ·»åŠ åŠ©æ‰‹å“åº”
            else:
                chat_history[-1]["content"] = response  # æ›´æ–°åŠ©æ‰‹å“åº”
            yield chat_history


    submit_btn.click(respond, [msg, chatbot], [chatbot])
    stop_btn.click(stop_generation_fn, None, None)
    clear_btn.click(model_history_restart, None, [chatbot])
    save_btn.click(save_history, None, [file_save])
    load_btn.upload(load_history, load_btn, [chatbot])
    temperature.change(update_config, [temperature, top_k, top_p, repeat_penalty, seed], None)
    top_k.change(update_config, [temperature, top_k, top_p, repeat_penalty, seed], None)
    top_p.change(update_config, [temperature, top_k, top_p, repeat_penalty, seed], None)
    repeat_penalty.change(update_config, [temperature, top_k, top_p, repeat_penalty, seed], None)
    seed.change(update_config, [temperature, top_k, top_p, repeat_penalty, seed], None)
    random_seed_btn.click(random_seed, None, [seed])

    # é¡µé¢åŠ è½½æ—¶æ¸…é™¤å†å²
    demo.load(model_history_restart, None, [chatbot])

# å…¥å£å‡½æ•°
if __name__ == '__main__':
    atexit.register(stop_at_exit)  # æ³¨å†Œé€€å‡º
    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    webbrowser.open_new_tab(f"http://{configs['host']}:{configs['uiport']}")  # æ‰“å¼€é»˜è®¤æµè§ˆå™¨çš„æ–°æ ‡ç­¾é¡µ
    # å¯åŠ¨ï¼Œè¿è¡Œåœ¨0.0.0.0æœ¬æœºä¸Šï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿç¯å¢ƒå˜é‡GRADIO_SERVER_NAMEæ¥æ”¹å˜
    demo.launch(server_name='0.0.0.0', server_port=int(configs['uiport']))
